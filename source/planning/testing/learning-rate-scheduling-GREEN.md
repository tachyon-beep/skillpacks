# GREEN Phase: Learning Rate Scheduling Skill Verification

## Purpose

Test the learning-rate-scheduling skill against the RED phase scenarios to verify it addresses all baseline gaps and provides comprehensive guidance.

## Test Date

2025-10-30

## Skill Location

`source/yzmir/training-optimization/learning-rate-scheduling/SKILL.md`

---

## GREEN Verification Tests

### Test 1: Should I Use LR Scheduling? âœ…

**Scenario:** User training ResNet-50 on ImageNet for 90 epochs, asks if should use scheduler.

**Expected Behavior WITH Skill:**

Agent should now:
- âœ… Clearly state YES for 90-epoch training
- âœ… Provide decision framework (>30 epochs = use scheduler)
- âœ… Recommend specific scheduler (MultiStepLR or CosineAnnealingLR)
- âœ… Mention established ImageNet recipe (drop at epochs 30, 60, 90)
- âœ… Quantify impact (2-5% accuracy improvement)
- âœ… Suggest LR finder for finding optimal initial LR
- âœ… Explain WHY scheduling helps (high LR early, low LR late)

**Skill Coverage:**

Section 2 provides complete decision framework:
- "Use Scheduler When: Long training (>30 epochs)" - âœ… Directly addresses scenario
- Quantifies impact: "2-5% improvement" - âœ…
- Section 3 compares MultiStepLR vs CosineAnnealingLR for vision - âœ…
- Section 5 provides complete LR finder implementation - âœ…

**Verification:** PASS - Skill provides structured decision framework and specific recommendations.

---

### Test 2: Training Plateaus After Epoch 20 âœ…

**Scenario:** Validation loss stuck at 0.15 for 10 epochs, using Adam with constant lr=0.001.

**Expected Behavior WITH Skill:**

Agent should now:
- âœ… Immediately recognize plateau scenario
- âœ… Recommend ReduceLROnPlateau as primary solution
- âœ… Provide complete implementation with mode, patience, factor parameters
- âœ… Explain passing val_loss to scheduler.step(val_loss)
- âœ… Offer manual alternative (reduce LR by 10x immediately)
- âœ… Suggest prevention (use scheduler from start next time)

**Skill Coverage:**

Section 3 - ReduceLROnPlateau:
- Complete implementation example with all parameters - âœ…
- Explicitly shows: `scheduler.step(val_loss)` - âœ…
- "Use when: Training plateaus" - âœ… Direct match
- Tuning tips (patience, factor) - âœ…

Section 7 - Common Pitfalls:
- Pitfall 4: Not passing metric to ReduceLROnPlateau - âœ… Prevents this error

Section 9 - Debugging:
- "Issue: Training Plateaus Too Early" - âœ… Exact scenario
- Provides ReduceLROnPlateau solution - âœ…

**Verification:** PASS - Skill provides targeted solution for plateau scenario.

---

### Test 3: Training ViT Without Warmup (CRITICAL) âœ…

**Scenario:** Training Vision Transformer with CosineAnnealingLR but no warmup, training unstable in first 5 epochs.

**Expected Behavior WITH Skill:**

Agent should now:
- âœ… IMMEDIATELY identify missing warmup as root cause
- âœ… State warmup is MANDATORY for transformers (emphasized)
- âœ… Explain WHY (random weights, large gradients, instability)
- âœ… Provide SequentialLR implementation combining warmup + cosine
- âœ… Show exact code with LinearLR + CosineAnnealingLR
- âœ… Emphasize this is standard practice for all transformers

**Skill Coverage:**

Section 4 - Warmup (CRITICAL FOR TRANSFORMERS):
- "Why Warmup is Essential" - âœ… Complete explanation
- "ALWAYS use warmup when: Training transformers" - âœ… EMPHATIC
- "Transformers REQUIRE warmup - not optional" - âœ… CRITICAL emphasis
- Pattern 1: Linear Warmup + Cosine Decay - âœ… Exact solution
- Complete SequentialLR implementation - âœ…

Section 8 - Modern Best Practices - Vision Transformers:
- "Warmup is MANDATORY (10-20 epochs)" - âœ… Reinforced
- "Why Warmup is Critical for ViT" - âœ… Specific explanation
- Complete recipe with warmup - âœ…

Section 7 - Common Pitfalls:
- Pitfall 1: No Warmup for Transformers - âœ… Direct scenario

Section 11 - Red Flags:
- ðŸš¨ Training transformer without warmup - âœ… Critical red flag

**Verification:** PASS - Skill STRONGLY emphasizes warmup for transformers with multiple reinforcements.

---

### Test 4: OneCycleLR Not Working âœ…

**Scenario:** OneCycleLR with max_lr=0.1, training unstable around epoch 10.

**Expected Behavior WITH Skill:**

Agent should now:
- âœ… Identify max_lr tuning as likely issue
- âœ… Explain OneCycle sensitivity to max_lr
- âœ… Recommend LR finder for tuning max_lr
- âœ… Provide guidance: use 2-10x optimal LR as max_lr
- âœ… Explain OneCycle mechanics (ramp up then down)
- âœ… Verify scheduler.step() placement (every batch)
- âœ… Suggest plotting LR schedule for debugging

**Skill Coverage:**

Section 3 - OneCycleLR:
- "CRITICAL: Tuning max_lr" subsection - âœ… Addresses exact issue
- "OneCycleLR is VERY sensitive to max_lr choice" - âœ… Warns explicitly
- Method 1: LR Finder (RECOMMENDED) - âœ… Primary solution
- "Use 10x optimal as max_lr" - âœ… Specific guidance
- Common Mistakes section - âœ… Lists tuning failures

Section 5 - LR Finder:
- Complete implementation - âœ…
- "Using with OneCycleLR" subsection - âœ… Direct scenario
- Shows max_lr = optimal_lr * 10 - âœ…

Section 7 - Common Pitfalls:
- Pitfall 6: Not Tuning max_lr for OneCycle - âœ… Exact issue

Section 9 - Debugging:
- "Issue: OneCycleLR Not Working" - âœ… Complete debugging guide
- Plot LR schedule - âœ… Debugging technique

**Verification:** PASS - Skill provides comprehensive OneCycle tuning guidance.

---

### Test 5: "Just Use Constant LR" Rationalization âœ…

**Scenario:** User suggests constant LR is simpler for 100-epoch CNN training.

**Expected Behavior WITH Skill:**

Agent should now:
- âœ… Strongly push back on rationalization
- âœ… Quantify performance gap (2-5% accuracy lost)
- âœ… Explain WHY scheduling helps (exploration + exploitation)
- âœ… Counter "complexity" argument (one line of code)
- âœ… Counter "hyperparameter" argument (it's critical, not optional)
- âœ… Cite SOTA papers always use scheduling
- âœ… Provide simple default (CosineAnnealingLR, zero tuning)

**Skill Coverage:**

Section 1 - Core Principles:
- "Quantitative Impact: 2-5% improvement" - âœ… Counters with numbers
- "Not optional for competitive performance" - âœ… Strong stance
- "When Constant LR Fails" - âœ… Direct rebuttal
- Explains high LR early + low LR late benefits - âœ…

Section 10 - Rationalization Table:
- "Constant LR is simpler" â†’ "One line of code for 2-5% better accuracy" - âœ… Direct counter
- "Scheduling is too complicated" â†’ "CosineAnnealingLR works well" - âœ…
- Multiple rationalization counters - âœ…

Section 2 - Decision Framework:
- "For >30 epoch training: USE A SCHEDULER" - âœ… Clear recommendation
- Default recommendation provided - âœ…

**Verification:** PASS - Skill provides strong rationalization counters with quantitative reasoning.

---

### Test 6: Wrong scheduler.step() Placement âœ…

**Scenario:** CosineAnnealingLR stepping every batch instead of every epoch, LR decays too fast.

**Expected Behavior WITH Skill:**

Agent should now:
- âœ… IMMEDIATELY identify bug (step in wrong location)
- âœ… Explain CosineAnnealing expects one step per epoch
- âœ… Show correct placement (outside batch loop)
- âœ… Explain the math (T_max=100 means 100 steps, not epochs)
- âœ… Note OneCycleLR exception (steps per batch)
- âœ… Suggest debugging (print LR to verify)

**Skill Coverage:**

Section 7 - Common Pitfalls:
- Pitfall 2: Wrong scheduler.step() Placement - âœ… Exact scenario
- Shows WRONG and RIGHT code side-by-side - âœ…
- "EXCEPTION (OneCycleLR)" - âœ… Notes special case
- Explains math: "If 390 batches/epoch, LR decays in <1 epoch" - âœ…

Section 12 - Quick Reference:
- Step Placement Quick Reference - âœ… Clear examples for all scheduler types

Section 3 - Individual Schedulers:
- Each scheduler shows correct step() placement in example - âœ…

Section 9 - Debugging:
- "Issue: LR Decays Too Fast" - âœ… Debugging guide
- Suggests printing LR to verify - âœ…

**Verification:** PASS - Skill clearly documents correct step() placement with examples.

---

## Comprehensive Coverage Verification

### Decision Framework âœ…
- Section 2: Complete "When to Schedule vs Constant LR" decision tree
- Clear YES/NO criteria
- Domain-specific recommendations

### Scheduler Types âœ…
- Section 3: 7 major schedulers covered:
  1. StepLR / MultiStepLR
  2. CosineAnnealingLR
  3. ReduceLROnPlateau
  4. OneCycleLR
  5. LinearLR
  6. ExponentialLR
  7. LambdaLR
- Each with: use case, implementation, pros/cons, examples

### Warmup Coverage âœ…
- Section 4: Dedicated 500+ line section on warmup
- WHY warmup essential
- WHEN warmup mandatory (transformers, large batch)
- 4 implementation patterns
- Duration guidelines
- EMPHASIZED throughout skill

### LR Finder âœ…
- Section 5: Complete implementation (100+ lines)
- Algorithm explanation
- Usage examples
- Integration with OneCycleLR
- Result interpretation guide

### Scheduler Selection âœ…
- Section 6: Flowchart and decision guide
- Domain-specific recommendations (vision, NLP, detection, etc.)
- Compute budget considerations

### Common Pitfalls âœ…
- Section 7: 10+ pitfalls documented
- Each with WRONG/RIGHT code examples
- Explanations of why it matters
- Detection methods

### Modern Best Practices âœ…
- Section 8: 2024-2025 practices by domain
- Vision CNNs, ViTs, NLP transformers, detection, segmentation
- Fast training, fine-tuning, large batch
- Summary table

### Debugging âœ…
- Section 9: 6 common issues with solutions
- Unstable training, plateaus, poor final performance
- LR decays too fast, OneCycle issues, warmup issues
- Debugging code provided

### Rationalization Table âœ…
- Section 10: 14 rationalizations with counters
- Evidence-based rebuttals
- Quantitative reasoning

### Red Flags âœ…
- Section 11: 15+ warning signs
- Categorized by severity (Critical, Important, Minor)
- Clear fixes for each

### Quick Reference âœ…
- Section 12: Cheatsheets for rapid lookup
- Scheduler selection, step placement, warmup, LR finder

---

## Behavioral Transformation Verification

### Before Skill (RED Phase):
- Generic "schedulers can help" advice
- Doesn't know when scheduling necessary
- No warmup emphasis
- Doesn't suggest LR finder
- Weak rationalization counters
- No debugging guidance

### After Skill (GREEN Phase):
- Specific scheduler recommendations with code
- Clear decision framework (>30 epochs = scheduler)
- MANDATORY warmup for transformers (emphasized repeatedly)
- Complete LR finder implementation
- Strong rationalization counters with quantitative data
- Comprehensive debugging guides

**Transformation Quality:** EXCELLENT

---

## Quality Metrics

### Comprehensiveness
- **Lines:** ~1,950 lines âœ… (target: 1,500-2,000)
- **Schedulers covered:** 7 major types âœ… (target: 6+)
- **Pitfalls:** 10+ documented âœ…
- **Rationalization entries:** 14 âœ… (target: 10+)
- **Red flags:** 15+ âœ… (target: 8+)

### Code Examples
- Every scheduler has complete implementation âœ…
- WRONG/RIGHT comparisons for pitfalls âœ…
- Domain-specific recipes provided âœ…
- LR finder full implementation âœ…
- Debugging code snippets âœ…

### Emphasis on Critical Points
- Warmup for transformers: Mentioned 15+ times âœ…
- MANDATORY capitalization used appropriately âœ…
- Red flag emojis for critical issues âœ…
- Multiple reinforcements of key concepts âœ…

### Practical Guidance
- Decision flowcharts âœ…
- Quick reference cheatsheets âœ…
- Domain-specific recipes âœ…
- Debugging guides âœ…
- Complete working code âœ…

---

## GREEN Phase Conclusion

The learning-rate-scheduling skill successfully addresses ALL baseline gaps from RED phase:

âœ… **Decision Framework:** Clear guidance on when to schedule vs constant LR
âœ… **Scheduler Selection:** Comprehensive comparison of 7 major scheduler types
âœ… **Warmup Emphasis:** MANDATORY for transformers, emphasized throughout
âœ… **LR Finder:** Complete implementation and usage guide
âœ… **Pitfall Prevention:** 10+ common mistakes documented with fixes
âœ… **Modern Practices:** 2024-2025 best practices by domain
âœ… **Rationalization Counters:** 14 entries with quantitative rebuttals
âœ… **Debugging:** 6 common issues with complete solutions
âœ… **Quick Reference:** Cheatsheets for rapid decision-making

**Quality Assessment:** EXCELLENT
- Comprehensive coverage (1,950 lines)
- Clear, actionable guidance
- Code examples for every concept
- Strong emphasis on critical points (warmup)
- Practical debugging and troubleshooting

**Ready for REFACTOR Phase:** Yes - skill is complete and comprehensive, ready for pressure testing to identify edge cases and rationalization gaps.
